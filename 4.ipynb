{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b63e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd08498",
   "metadata": {},
   "source": [
    "### Question.1. Consider a 3rd order polynomial:\n",
    "$f(x) = a_{0} + a_{1}x + a_{2}x^2 + a_{3}x^3$\n",
    "### Suppose we are given a set of samples of this function (xi, yi) for 1 <= i <= N.\n",
    "### The least squares residual is given by R = $\\sum_{n=1}^{10}(y_{i} - f(x_{i})^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc66dbe2",
   "metadata": {},
   "source": [
    "### Question.1.a. Using calculus, determine the values for a0, a1, a2, and a3 that minimize this residual. Hint 1: consider where the residual R is at a minimum. Hint 2: Try to write in matrix form in terms of a linear system of equations. Write a0, a1, a2, and a3 in those terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c27da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating x values for LSM\n",
    "xi_1 = np.linspace(0, 1, 51)\n",
    "\n",
    "#Calculating y values\n",
    "yi_1 = 1 + xi_1 + xi_1 ** 2 + xi_1 **3\n",
    "\n",
    "#print(yi_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Design matrix A\n",
    "A = np.column_stack([np.ones_like(xi_1), xi_1, xi_1 ** 2, xi_1 ** 3]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the values for coefficients\n",
    "coefficients = np.linalg.inv(A @ A.T) @ A @ yi_1\n",
    "print(\"Coefficients:\")\n",
    "print(f\"a0:{coefficients[0]}\")\n",
    "print(f\"a1:{coefficients[1]}\")\n",
    "print(f\"a2:{coefficients[2]}\")\n",
    "print(f\"a3:{coefficients[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd489d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using coefficients to construct the polynomial\n",
    "yi_1_hat = 1 + coefficients[0] * xi_1 + coefficients[1] * xi_1 ** 2 + coefficients[2] * xi_1 ** 3\n",
    "print(yi_1_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of xi_1 and yi_1 and yi_1_hat\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(xi_1, yi_1, 'b.', label = \"True values\")\n",
    "plt.plot(xi_1, yi_1_hat, 'r', label = \"Least Square\")\n",
    "plt.xlabel(\"x[i]\")\n",
    "plt.ylabel(\"y[i]\")\n",
    "plt.title(\"Plot of x[i] with best coefficients\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d772cfc6",
   "metadata": {},
   "source": [
    "### Question.2.Consider the function f(x) = xsin(x). Write code to take samples of this function (at least 50). Add normally-distributed random noise to the function (e.g. use np.random.normal). \n",
    "### Reconstruct the function using the given samples using polynomial models as we did in class. Try this reconstruction over at least 5 different polynomial orders (suggest linear up to 5th order). Which order of polynomial gives the best reconstruction and why? Use the model assessment methodology we have discussed in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89eb4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function with added noise\n",
    "def funct(x):\n",
    "    return((x * np.sin(x)) + np.random.normal(scale = 1, size = (len(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the function to generate at least 50 samples\n",
    "x_2 = np.arange(0, 100, 1)\n",
    "#print(x_2)\n",
    "\n",
    "y_2 = funct(x_2)\n",
    "print(f\"Values of y for function f(x) = x * sin(x): {y_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19562cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the generated values\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x_2, y_2)\n",
    "plt.title(\"Plot for function f(x) = x * sin(x) + random noise\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbac049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruction \n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "x_2_train = np.linspace(0, 100, 100)\n",
    "x_2_train = np.sort(rng.choice(x_2_train, size = 80, replace = False))\n",
    "y_2_train = funct(x_2_train)\n",
    "\n",
    "x_2 = x_2[:,np.newaxis]\n",
    "x_2_train = x_2_train[:,np.newaxis]\n",
    "\n",
    "\n",
    "#print(x_2)\n",
    "#print(x_2_train)\n",
    "#print(y_2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d8114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruction over at least 5 different polynomial orders\n",
    "\n",
    "lw =1 \n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle(color = [\"black\",\"green\",\"red\",\"gold\",\"teal\",\"pink\"])\n",
    "ax.plot(x_2, y_2, linewidth = lw, label = \"Truth\")\n",
    "ax.scatter(x_2_train, y_2_train, label = \"Training\")\n",
    "\n",
    "for degree in [1,2,3,4,5]:\n",
    "    poly_model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    poly_model.fit(x_2_train, y_2_train)\n",
    "    y_2_predict = poly_model.predict(x_2)\n",
    "    ax.plot(x_2, y_2_predict, label = f\"degree: {degree}\")\n",
    "    r2_sc = r2_score(y_2, y_2_predict)\n",
    "    \n",
    "plt.title(\"Plot of Polynomial of degree 5\")    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47358c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48095848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b016e82e",
   "metadata": {},
   "source": [
    "### Question.3. Consider a model fo the form:\n",
    "f(x) = 1 + x + $x^2$ + $\\log _{}x $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8b0c63",
   "metadata": {},
   "source": [
    "### Question.3.a. Plot this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec006ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate values for x\n",
    "\n",
    "x_3 = np.arange(1,50,1)\n",
    "print(f\"Generated values for x: {x_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_3 = 1 + x_3 + (x_3 ** 2) + np.log(x_3)\n",
    "print(f\"Calculated values for y: {y_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec96a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(x_3, y_3, label = \"f(x) = 1 + x + x^2 + log(x)\")\n",
    "plt.plot(x_3, y_3, \"r.\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Plot of f(x) = 1 + x + x^2 + log(x)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822f8fc9",
   "metadata": {},
   "source": [
    "### Question.3.b. Generate a number of samples of this function and plot it. Add some random, normally-distributed noise to this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate random normally distributed noise\n",
    "\n",
    "noise = np.random.normal(scale = 4, size = (len(x_3)))\n",
    "print(f\"Values for noise: {noise}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5d0210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate number of samples\n",
    "print(f\"-----> y values: {y_3}\")\n",
    "new_y = y_3 + noise\n",
    "print(f\"-----> y values with noise: {new_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the values of new_y\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(x_3, new_y , label = \"f(x) = 1 + x + x^2 + log(x) + noise\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y with noise\")\n",
    "plt.title(\"Plot of f(x) = 1 + x + x^2 + log(x) + noise\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Plot of original y\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(x_3, y_3, \"-r\", label = \"f(x) = 1 + x + x^2 + log(x)\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Plot of f(x) = 1 + x + x^2 + log(x)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc8bf5e",
   "metadata": {},
   "source": [
    "### Question.3.c. Reconstruct the original function from samples of the random noisey function in scikit learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed89e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to generate samples of the random noisy function\n",
    "def generate_noise(x, add_noise = False):\n",
    "    rng = np.random.RandomState(1)\n",
    "    fun = 1 + x + (x ** 2) + np.log(x)\n",
    "    if add_noise:\n",
    "        fun += rng.normal(0, 0.3, size = fun.shape)\n",
    "    return fun.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e1b0b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_y_with_noise = generate_noise(x_3, add_noise = True)\n",
    "print(f\"Values of y from samples of the random noisey function: {new_y_with_noise}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9405cfe",
   "metadata": {},
   "source": [
    "### Question.2.d.i. Considering your knowledge here, What model would best work here? Implement this using Scikit-learn.  Compare your choice of model to a linear and k-NN model. Searching over k for best value for your comparison. Show plots of these functions (including the ground truth) simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe47f31",
   "metadata": {},
   "source": [
    "#### A polynomial regression model might be suitable for this scrnarioas the nature of the generated data is with polynomial features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9822d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Polynomial regression, Linear Regression model and knn regression and fitting with noisy data\n",
    "\n",
    "\n",
    "x_3_reshaped = x_3.reshape(-1,1)\n",
    "\n",
    "#Polynomial Regression\n",
    "param_grid_poly = {'polynomialfeatures__degree': np.arange(1, 6)}\n",
    "poly_reg = make_pipeline(PolynomialFeatures(), LinearRegression())\n",
    "grid_poly = GridSearchCV(poly_reg, param_grid_poly, cv=5)\n",
    "grid_poly.fit(x_3.reshape(-1, 1), new_y_with_noise)\n",
    "best_degree_poly = grid_poly.best_params_['polynomialfeatures__degree']\n",
    "\n",
    "# Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(x_3.reshape(-1, 1), new_y_with_noise)\n",
    "\n",
    "#k-NN Regression\n",
    "param_grid_knn = {'n_neighbors': np.arange(1, 21)}\n",
    "knn_reg = KNeighborsRegressor()\n",
    "grid_knn = GridSearchCV(knn_reg, param_grid_knn, cv=5)\n",
    "grid_knn.fit(x_3.reshape(-1, 1), new_y_with_noise)\n",
    "best_k_knn = grid_knn.best_params_['n_neighbors']\n",
    "\n",
    "x_plot_val = np.linspace(0.1, 50, 49).reshape(-1, 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle(color=[\"black\",\"teal\",\"yellowgreen\",\"gold\",\"blue\",\"red\"])\n",
    "ax.plot(x_plot_val,y_3,linewidth=1,label=\"Truth\")\n",
    "ax.scatter(x_plot_val, new_y_with_noise, label =\"Training data\")\n",
    "\n",
    "for degree in [1,2,3,4,5]:\n",
    "    poly_reg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    lin_reg = LinearRegression()\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    poly_reg.fit(x_3_reshaped, new_y_with_noise)\n",
    "    lin_reg.fit(x_3_reshaped, new_y_with_noise)\n",
    "    knn_reg.fit(x_3_reshaped, new_y_with_noise)\n",
    "    #Predicting y values\n",
    "    y_poly_pred = poly_reg.predict(x_plot_val)\n",
    "    y_lin_pred = lin_reg.predict(x_plot_val)\n",
    "    y_knn_pred = knn_reg.predict(x_plot_val)\n",
    "    #Generating plots\n",
    "    ax.plot(x_plot_val, y_poly_pred, label = f\"Polynomial (degree {degree})\")\n",
    "    \n",
    "ax.plot(x_plot_val, y_knn_pred, label=f\"k-NN Regression (k={best_k_knn})\", linestyle='dashed', color='orange')\n",
    "print(f\"Best value of k for k-NN: {best_k_knn}\")\n",
    "\n",
    "plt.title(\"Plot of Polynomial\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57269bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting new values and getting accuracy values\n",
    "\n",
    "# Polynomial Regression\n",
    "y_poly_pred = grid_poly.predict(x_3.reshape(-1, 1))\n",
    "mse_poly = mean_squared_error(new_y_with_noise, y_poly_pred)\n",
    "r2_poly = r2_score(new_y_with_noise, y_poly_pred)\n",
    "\n",
    "# Linear Regression\n",
    "y_lin_pred = lin_reg.predict(x_3.reshape(-1, 1))\n",
    "mse_lin = mean_squared_error(new_y_with_noise, y_lin_pred)\n",
    "r2_lin = r2_score(new_y_with_noise, y_lin_pred)\n",
    "\n",
    "# k-NN Regression\n",
    "y_knn_pred = grid_knn.predict(x_3.reshape(-1, 1))\n",
    "mse_knn = mean_squared_error(new_y_with_noise, y_knn_pred)\n",
    "r2_knn = r2_score(new_y_with_noise, y_knn_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean Squared Error (MSE) and R-squared (R2) for each model:\")\n",
    "print(f\"Polynomial Regression (degree {best_degree_poly}):\")\n",
    "print(f\"  MSE: {mse_poly:.4f}, R2: {r2_poly:.4f}\")\n",
    "\n",
    "print(\"\\nLinear Regression:\")\n",
    "print(f\"  MSE: {mse_lin:.4f}, R2: {r2_lin:.4f}\")\n",
    "\n",
    "print(\"\\nk-NN Regression (k={best_k_knn}):\")\n",
    "print(f\"  MSE: {mse_knn:.4f}, R2: {r2_knn:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
